{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "> Efficient training tricks like Sequence Packing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dart_math.train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelerating Several Times with Sequence Packing in 4 Lines of Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our interfaces can be integrated with the [HuggingFace `datasets`](https://huggingface.co/docs/datasets/en/index) in 4 lines of code:\n",
    "\n",
    "```python\n",
    "from dart_math.train import monkey_patch4pack, make_supervised_dset\n",
    "# ...\n",
    "monkey_patch4pack(model)\n",
    "pack_dset = make_supervised_dset(tokenizer=tokenizer, data_path=data_args.data_path, pack_len=training_args.model_max_length, query_field=data_args.query_field,, resp_field=data_args.resp_field,, prompt_template=data_args.prompt_template)\n",
    "trainer = Trainer(model=model, tokenizer=tokenizer, train_dataset=pack_dset)\n",
    "```\n",
    "\n",
    "`monkey_patch4pack` would monkey-patch the model's `_get_unpad_data` method.\n",
    "\n",
    "`make_supervised_dset` would\n",
    "\n",
    "1. load, tokenize and cache the dataset;\n",
    "2. pack the data points into computation sequences.\n",
    "\n",
    "For a more detailed usage example, please refer to our [training script for DART-Math](https://github.com/hkust-nlp/dart-math/blob/main/pipeline/train.py).\n",
    "\n",
    "Besides, for general datasets objects that with the form `[{\"input_ids\": [...], \"labels\": [...], \"attention_mask\"}: [...]}, ...]`, you can use `PackedDataset` to wrap it to apply sequence packing:\n",
    "\n",
    "```python\n",
    "from dart_math.train import PackedDataset\n",
    "# ...\n",
    "dset = PackedDataset(dataset=dset, tokenizer=tokenizer, pack_len=4096)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### monkey_patch4pack\n",
       "\n",
       ">      monkey_patch4pack (name_or_cls_or_obj:str|type|transformers.configuration\n",
       ">                         _utils.PretrainedConfig)\n",
       "\n",
       "*Monkey patch the modeling module for packing. Must be called before instantiating the model.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| name_or_cls_or_obj | str \\| type \\| transformers.configuration_utils.PretrainedConfig | Name containing the model name like \"llama\" / \"mistral\" / ... |\n",
       "| **Returns** | **None** |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### monkey_patch4pack\n",
       "\n",
       ">      monkey_patch4pack (name_or_cls_or_obj:str|type|transformers.configuration\n",
       ">                         _utils.PretrainedConfig)\n",
       "\n",
       "*Monkey patch the modeling module for packing. Must be called before instantiating the model.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| name_or_cls_or_obj | str \\| type \\| transformers.configuration_utils.PretrainedConfig | Name containing the model name like \"llama\" / \"mistral\" / ... |\n",
       "| **Returns** | **None** |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(monkey_patch4pack, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### make_supervised_dset\n",
       "\n",
       ">      make_supervised_dset\n",
       ">                            (tokenizer:transformers.tokenization_utils.PreTrain\n",
       ">                            edTokenizer, data_path:str|list[str],\n",
       ">                            query_field:str|list[str]='query',\n",
       ">                            resp_field:str|list[str]='response', tokenized_cach\n",
       ">                            e_home:str='/ssddata/tongyx/repos/dart-\n",
       ">                            math/data/cache-tokenized', shuffle_seed:int=42,\n",
       ">                            pack_len:int=None, prompt_template:str|dict[str,str\n",
       ">                            ]|dart_math.utils.PromptTemplate='alpaca')\n",
       "\n",
       "*Make dataset for supervised fine-tuning.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer | PreTrainedTokenizer |  | (HF) tokenizer. |\n",
       "| data_path | str \\| list[str] |  | Dataset ID or path. |\n",
       "| query_field | str \\| list[str] | query | Field name for query. |\n",
       "| resp_field | str \\| list[str] | response | Field name for response. |\n",
       "| tokenized_cache_home | str | /ssddata/tongyx/repos/dart-math/data/cache-tokenized | Path to the tokenized cache home. Useful when repeatedly training on large datasets. None or \"\" means no cache. |\n",
       "| shuffle_seed | int | 42 | Seed for shuffling the dataset before packing. None or negative means no shuffling. |\n",
       "| pack_len | int | None | Maximum length of packed computation sequence in token. None / Non-positive means no packing. |\n",
       "| prompt_template | str \\| dict[str, str] \\| dart_math.utils.PromptTemplate | alpaca | ID / File path / PromptTemplate object of prompt template. |\n",
       "| **Returns** | **dart_math.train.TokenizedSupervisedDataset \\| dart_math.train.PackedDataset** |  | **Dataset ready for input to `Trainer`, containing the following fields at least: `\"input_ids\"`, `\"labels\"`, and `\"attention_mask\"`.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### make_supervised_dset\n",
       "\n",
       ">      make_supervised_dset\n",
       ">                            (tokenizer:transformers.tokenization_utils.PreTrain\n",
       ">                            edTokenizer, data_path:str|list[str],\n",
       ">                            query_field:str|list[str]='query',\n",
       ">                            resp_field:str|list[str]='response', tokenized_cach\n",
       ">                            e_home:str='/ssddata/tongyx/repos/dart-\n",
       ">                            math/data/cache-tokenized', shuffle_seed:int=42,\n",
       ">                            pack_len:int=None, prompt_template:str|dict[str,str\n",
       ">                            ]|dart_math.utils.PromptTemplate='alpaca')\n",
       "\n",
       "*Make dataset for supervised fine-tuning.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer | PreTrainedTokenizer |  | (HF) tokenizer. |\n",
       "| data_path | str \\| list[str] |  | Dataset ID or path. |\n",
       "| query_field | str \\| list[str] | query | Field name for query. |\n",
       "| resp_field | str \\| list[str] | response | Field name for response. |\n",
       "| tokenized_cache_home | str | /ssddata/tongyx/repos/dart-math/data/cache-tokenized | Path to the tokenized cache home. Useful when repeatedly training on large datasets. None or \"\" means no cache. |\n",
       "| shuffle_seed | int | 42 | Seed for shuffling the dataset before packing. None or negative means no shuffling. |\n",
       "| pack_len | int | None | Maximum length of packed computation sequence in token. None / Non-positive means no packing. |\n",
       "| prompt_template | str \\| dict[str, str] \\| dart_math.utils.PromptTemplate | alpaca | ID / File path / PromptTemplate object of prompt template. |\n",
       "| **Returns** | **dart_math.train.TokenizedSupervisedDataset \\| dart_math.train.PackedDataset** |  | **Dataset ready for input to `Trainer`, containing the following fields at least: `\"input_ids\"`, `\"labels\"`, and `\"attention_mask\"`.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(make_supervised_dset, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Packing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Packing Accelerates 6-8x than Simple Batching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple batching** that pad every data sequence to the maximum training length wastes a lot computation and memory on padding tokens, especially for short data sequences and long maximum training length.\n",
    "\n",
    "For example, if the model maximum training length is 4096 (as in most base models like Mistral-7B and the longest data sequences in some datasets like MATH), and data sequences are ~512 tokens long on average (as in most math SFT datasets), we **waste almost 1-1/8=7/8 computation and memory on padding tokens**.\n",
    "\n",
    "**Sequence packing can eliminate the waste almost completely, without affecting the training dynamics** (for most models nowadays), except for the number of data sequences in one batch .\n",
    "\n",
    "In the example above, we can **accelerate about 6-8x** with sequence packing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Idea of Sequence Packing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of sequence packing is\n",
    "\n",
    "- to **merge/pack short data sequences into a single conputation sequence as long as the maximum training length** to **eliminate most watse on padding tokens**,\n",
    "- while trying best to **not affecting the training dynamics** by\n",
    "  - manipulating **attention masks** to avoid cross-contamination between different data sequences,\n",
    "  - working with **relative positional encoding** to avoid the positional information mismatch for the non-first data sequences in the packed computation sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulating Attention Masks to Avoid Cross-Contamination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .container {\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "    }\n",
    "    .container img {\n",
    "        height: 200px; /* Set the desired height */\n",
    "        object-fit: cover; /* Maintains aspect ratio */\n",
    "    }\n",
    "    .caption {\n",
    "        text-align: center;\n",
    "        font-size: small;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "</style>\n",
    "<div class=\"container\">\n",
    "<img src=\"https://github.com/MeetKai/functionary/blob/main/functionary/train/packing/assets/cross_contamination.png?raw=true\">\n",
    "<img src=\"https://github.com/MeetKai/functionary/blob/main/functionary/train/packing/assets/correct_packing_attention.png?raw=true\">\n",
    "</div>\n",
    "\n",
    "> Concretely, when we pack inputs, the attention should be only within individual sequences. For example, assume that we are packing 2 inputs: packed input = [input 1] [input 2]. Tokens from **input 1** only attend to tokens from **input 1** and tokens from **input 2** only attend to tokens from **input 2**\n",
    ">\n",
    "> Examples of packing 2 input sequences: \"good morning my name is John\" and \"This is a dog\". The first one is the attention matrix of packing with cross-contamination, the second one is the correct attention matrix of packing.\n",
    ">\n",
    "> c.f. https://github.com/MeetKai/functionary/tree/main/functionary/train/packing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative Positinal Encoding Perferctly Works with Sequence Packing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, sequence packing introduces another problem: **the positional encodings of the non-first data sequences in one computation sequence are not the same as the vanilla non-packing setting**.\n",
    "\n",
    "This is indeed a problem for absolute positional encoding, but practically **does not matter for relative positional encoding** like [RoPE](https://arxiv.org/abs/2104.09864), which is almost the de facto practice nowadays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PackedDataset\n",
       "\n",
       ">      PackedDataset\n",
       ">                     (dataset:torch.utils.data.dataset.Dataset|datasets.arrow_d\n",
       ">                     ataset.Dataset, tokenizer:transformers.tokenization_utils.\n",
       ">                     PreTrainedTokenizer, pack_len:int, shuffle_seed:int=42)\n",
       "\n",
       "*Packed dataset containing computation sequences.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset | torch.utils.data.dataset.Dataset \\| datasets.arrow_dataset.Dataset |  | Original tokenized dataset, which should have the following fields at least: `\"input_ids\"`, `\"labels\"`, and `\"attention_mask\"`. |\n",
       "| tokenizer | PreTrainedTokenizer |  | (HF) tokenizer. |\n",
       "| pack_len | int |  | Maximum length of packed compuation sequence in token. |\n",
       "| shuffle_seed | int | 42 | Seed for shuffling the dataset before packing. `None` / Negative values mean no shuffling. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### PackedDataset\n",
       "\n",
       ">      PackedDataset\n",
       ">                     (dataset:torch.utils.data.dataset.Dataset|datasets.arrow_d\n",
       ">                     ataset.Dataset, tokenizer:transformers.tokenization_utils.\n",
       ">                     PreTrainedTokenizer, pack_len:int, shuffle_seed:int=42)\n",
       "\n",
       "*Packed dataset containing computation sequences.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset | torch.utils.data.dataset.Dataset \\| datasets.arrow_dataset.Dataset |  | Original tokenized dataset, which should have the following fields at least: `\"input_ids\"`, `\"labels\"`, and `\"attention_mask\"`. |\n",
       "| tokenizer | PreTrainedTokenizer |  | (HF) tokenizer. |\n",
       "| pack_len | int |  | Maximum length of packed compuation sequence in token. |\n",
       "| shuffle_seed | int | 42 | Seed for shuffling the dataset before packing. `None` / Negative values mean no shuffling. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(PackedDataset, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### PackedDataset.stat\n",
       "\n",
       ">      PackedDataset.stat ()\n",
       "\n",
       "*Print out the statistics of the packed dataset.\n",
       "Original -> Packed:\n",
       "1. Number of data/computation sequences;\n",
       "2. Average effective length of compution sequences.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### PackedDataset.stat\n",
       "\n",
       ">      PackedDataset.stat ()\n",
       "\n",
       "*Print out the statistics of the packed dataset.\n",
       "Original -> Packed:\n",
       "1. Number of data/computation sequences;\n",
       "2. Average effective length of compution sequences.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(PackedDataset.stat, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TokenizedSupervisedDataset\n",
       "\n",
       ">      TokenizedSupervisedDataset\n",
       ">                                  (tokenizer:transformers.tokenization_utils.Pr\n",
       ">                                  eTrainedTokenizer,\n",
       ">                                  input_ids:list[torch.Tensor]=None,\n",
       ">                                  labels:list[torch.Tensor]=None,\n",
       ">                                  attention_mask:list[torch.Tensor]=None)\n",
       "\n",
       "*Tokenized dataset for supervised fine-tuning.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer | PreTrainedTokenizer |  | (HF) tokenizer. `None` for empty dataset. |\n",
       "| input_ids | list | None | List of input token ID sequences. |\n",
       "| labels | list | None | List of label sequences. |\n",
       "| attention_mask | list | None | List of attention mask sequences. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### TokenizedSupervisedDataset\n",
       "\n",
       ">      TokenizedSupervisedDataset\n",
       ">                                  (tokenizer:transformers.tokenization_utils.Pr\n",
       ">                                  eTrainedTokenizer,\n",
       ">                                  input_ids:list[torch.Tensor]=None,\n",
       ">                                  labels:list[torch.Tensor]=None,\n",
       ">                                  attention_mask:list[torch.Tensor]=None)\n",
       "\n",
       "*Tokenized dataset for supervised fine-tuning.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer | PreTrainedTokenizer |  | (HF) tokenizer. `None` for empty dataset. |\n",
       "| input_ids | list | None | List of input token ID sequences. |\n",
       "| labels | list | None | List of label sequences. |\n",
       "| attention_mask | list | None | List of attention mask sequences. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TokenizedSupervisedDataset, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### TokenizedSupervisedDataset.load_from_raw_dset\n",
       "\n",
       ">      TokenizedSupervisedDataset.load_from_raw_dset\n",
       ">                                                     (tokenizer:transformers.to\n",
       ">                                                     kenization_utils.PreTraine\n",
       ">                                                     dTokenizer, data_path:str,\n",
       ">                                                     query_field:str='query',\n",
       ">                                                     resp_field:str='response',\n",
       ">                                                     prompt_template:str|dict[s\n",
       ">                                                     tr,str]|dart_math.utils.Pr\n",
       ">                                                     omptTemplate='alpaca')\n",
       "\n",
       "*Load a dataset from a file and tokenize it.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer | PreTrainedTokenizer |  | (HF) tokenizer. |\n",
       "| data_path | str |  | Dataset ID or path. |\n",
       "| query_field | str | query | Field name for query. |\n",
       "| resp_field | str | response | Field name for response. |\n",
       "| prompt_template | str \\| dict[str, str] \\| dart_math.utils.PromptTemplate | alpaca | ID / File path / PromptTemplate object of prompt template. |\n",
       "| **Returns** | **TokenizedSupervisedDataset** |  |  |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### TokenizedSupervisedDataset.load_from_raw_dset\n",
       "\n",
       ">      TokenizedSupervisedDataset.load_from_raw_dset\n",
       ">                                                     (tokenizer:transformers.to\n",
       ">                                                     kenization_utils.PreTraine\n",
       ">                                                     dTokenizer, data_path:str,\n",
       ">                                                     query_field:str='query',\n",
       ">                                                     resp_field:str='response',\n",
       ">                                                     prompt_template:str|dict[s\n",
       ">                                                     tr,str]|dart_math.utils.Pr\n",
       ">                                                     omptTemplate='alpaca')\n",
       "\n",
       "*Load a dataset from a file and tokenize it.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| tokenizer | PreTrainedTokenizer |  | (HF) tokenizer. |\n",
       "| data_path | str |  | Dataset ID or path. |\n",
       "| query_field | str | query | Field name for query. |\n",
       "| resp_field | str | response | Field name for response. |\n",
       "| prompt_template | str \\| dict[str, str] \\| dart_math.utils.PromptTemplate | alpaca | ID / File path / PromptTemplate object of prompt template. |\n",
       "| **Returns** | **TokenizedSupervisedDataset** |  |  |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TokenizedSupervisedDataset.load_from_raw_dset, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### TokenizedSupervisedDataset.__getitem__\n",
       "\n",
       ">      TokenizedSupervisedDataset.__getitem__ (i:int)\n",
       "\n",
       "*Get a data point.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| i | int | `dataset[i]` |\n",
       "| **Returns** | **dict** | **`{\"input_ids\": input_ids[i], \"labels\": labels[i], \"attention_mask\": attention_mask[i]}`** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### TokenizedSupervisedDataset.__getitem__\n",
       "\n",
       ">      TokenizedSupervisedDataset.__getitem__ (i:int)\n",
       "\n",
       "*Get a data point.*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| i | int | `dataset[i]` |\n",
       "| **Returns** | **dict** | **`{\"input_ids\": input_ids[i], \"labels\": labels[i], \"attention_mask\": attention_mask[i]}`** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TokenizedSupervisedDataset.__getitem__, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### TokenizedSupervisedDataset.concat\n",
       "\n",
       ">      TokenizedSupervisedDataset.concat\n",
       ">                                         (datasets:list['TokenizedSupervisedDat\n",
       ">                                         aset'])\n",
       "\n",
       "*Concatenate `TokenizedSupervisedDataset` instances to the current dataset.\n",
       "datasets : list[TokenizedSupervisedDataset]\n",
       "    List of tokenized datasets to concatenate.\n",
       "    Each dataset should have the following fields at least: `\"input_ids\"`, `\"labels\"`, and `\"attention_mask\"`.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### TokenizedSupervisedDataset.concat\n",
       "\n",
       ">      TokenizedSupervisedDataset.concat\n",
       ">                                         (datasets:list['TokenizedSupervisedDat\n",
       ">                                         aset'])\n",
       "\n",
       "*Concatenate `TokenizedSupervisedDataset` instances to the current dataset.\n",
       "datasets : list[TokenizedSupervisedDataset]\n",
       "    List of tokenized datasets to concatenate.\n",
       "    Each dataset should have the following fields at least: `\"input_ids\"`, `\"labels\"`, and `\"attention_mask\"`.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TokenizedSupervisedDataset.concat, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### TokenizedSupervisedDataset.shuffle\n",
       "\n",
       ">      TokenizedSupervisedDataset.shuffle (seed:int=42)\n",
       "\n",
       "*Shuffle the dataset.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### TokenizedSupervisedDataset.shuffle\n",
       "\n",
       ">      TokenizedSupervisedDataset.shuffle (seed:int=42)\n",
       "\n",
       "*Shuffle the dataset.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TokenizedSupervisedDataset.shuffle, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### TokenizedSupervisedDataset.pad\n",
       "\n",
       ">      TokenizedSupervisedDataset.pad ()\n",
       "\n",
       "*Pad the dataset to the same length of the longest data point.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/train.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "#### TokenizedSupervisedDataset.pad\n",
       "\n",
       ">      TokenizedSupervisedDataset.pad ()\n",
       "\n",
       "*Pad the dataset to the same length of the longest data point.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(TokenizedSupervisedDataset.pad, title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowlegements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to https://github.com/MeetKai/functionary/tree/main/functionary/train/packing. The code for sequence packing is largely based on it.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
