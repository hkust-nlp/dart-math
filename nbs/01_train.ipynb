{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "> Efficient training tricks like Sequence Packing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dart_math.train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelerating Several Times with Sequence Packing in 4 Lines of Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our interfaces can be integrated with the [HuggingFace `datasets`](https://huggingface.co/docs/datasets/en/index) in 4 lines of code:\n",
    "\n",
    "```python\n",
    "from dart_math.train import monkey_patch4pack, make_supervised_dset\n",
    "# ...\n",
    "monkey_patch4pack(model)\n",
    "pack_dset = make_supervised_dset(tokenizer=tokenizer, data_path=data_args.data_path, pack_len=training_args.model_max_length, query_field=data_args.query_field,, resp_field=data_args.resp_field,, prompt_template=data_args.prompt_template)\n",
    "trainer = Trainer(model=model, tokenizer=tokenizer, train_dataset=pack_dset)\n",
    "```\n",
    "\n",
    "`monkey_patch4pack` would monkey-patch the model's `_get_unpad_data` method.\n",
    "\n",
    "`make_supervised_dset` would\n",
    "\n",
    "1. load, tokenize and cache the dataset;\n",
    "2. pack the data points into computation sequences.\n",
    "\n",
    "For a more detailed usage example, please refer to our [training script for DART-Math](https://github.com/hkust-nlp/dart-math/blob/main/pipeline/train.py).\n",
    "\n",
    "Besides, for general datasets objects that with the form `[{\"input_ids\": [...], \"labels\": [...], \"attention_mask\"}: [...]}, ...]`, you can use `PackedDataset` to wrap it to apply sequence packing:\n",
    "\n",
    "```python\n",
    "from dart_math.train import PackedDataset\n",
    "# ...\n",
    "dset = PackedDataset(dataset=dset, tokenizer=tokenizer, pack_len=4096)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(monkey_patch4pack, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(make_supervised_dset, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Packing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence Packing Accelerates 6-8x than Simple Batching\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple batching** that pad every data sequence to the maximum training length wastes a lot computation and memory on padding tokens, especially for short data sequences and long maximum training length.\n",
    "\n",
    "For example, if the model maximum training length is 4096 (as in most base models like Mistral-7B and the longest data sequences in some datasets like MATH), and data sequences are ~512 tokens long on average (as in most math SFT datasets), we **waste almost 1-1/8=7/8 computation and memory on padding tokens**.\n",
    "\n",
    "**Sequence packing can eliminate the waste almost completely, without affecting the training dynamics** (for most models nowadays), except for the number of data sequences in one batch .\n",
    "\n",
    "In the example above, we can **accelerate about 6-8x** with sequence packing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Idea of Sequence Packing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of sequence packing is\n",
    "\n",
    "- to **merge/pack short data sequences into a single conputation sequence as long as the maximum training length** to **eliminate most watse on padding tokens**,\n",
    "- while trying best to **not affecting the training dynamics** by\n",
    "  - manipulating **attention masks** to avoid cross-contamination between different data sequences,\n",
    "  - working with **relative positional encoding** to avoid the positional information mismatch for the non-first data sequences in the packed computation sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manipulating Attention Masks to Avoid Cross-Contamination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .container {\n",
    "        display: flex;\n",
    "        align-items: center;\n",
    "    }\n",
    "    .container img {\n",
    "        height: 200px; /* Set the desired height */\n",
    "        object-fit: cover; /* Maintains aspect ratio */\n",
    "    }\n",
    "    .caption {\n",
    "        text-align: center;\n",
    "        font-size: small;\n",
    "        margin-top: 10px;\n",
    "    }\n",
    "</style>\n",
    "<div class=\"container\">\n",
    "<img src=\"https://github.com/MeetKai/functionary/blob/main/functionary/train/packing/assets/cross_contamination.png?raw=true\">\n",
    "<img src=\"https://github.com/MeetKai/functionary/blob/main/functionary/train/packing/assets/correct_packing_attention.png?raw=true\">\n",
    "</div>\n",
    "\n",
    "> Concretely, when we pack inputs, the attention should be only within individual sequences. For example, assume that we are packing 2 inputs: packed input = [input 1] [input 2]. Tokens from **input 1** only attend to tokens from **input 1** and tokens from **input 2** only attend to tokens from **input 2**\n",
    ">\n",
    "> Examples of packing 2 input sequences: \"good morning my name is John\" and \"This is a dog\". The first one is the attention matrix of packing with cross-contamination, the second one is the correct attention matrix of packing.\n",
    ">\n",
    "> c.f. https://github.com/MeetKai/functionary/tree/main/functionary/train/packing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relative Positinal Encoding Perferctly Works with Sequence Packing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, sequence packing introduces another problem: **the positional encodings of the non-first data sequences in one computation sequence are not the same as the vanilla non-packing setting**.\n",
    "\n",
    "This is indeed a problem for absolute positional encoding, but practically **does not matter for relative positional encoding** like [RoPE](https://arxiv.org/abs/2104.09864), which is almost the de facto practice nowadays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(PackedDataset, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(PackedDataset.stat, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TokenizedSupervisedDataset, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TokenizedSupervisedDataset.load_from_raw_dset, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TokenizedSupervisedDataset.__getitem__, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TokenizedSupervisedDataset.concat, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TokenizedSupervisedDataset.shuffle, title_level=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_doc(TokenizedSupervisedDataset.pad, title_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowlegements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to https://github.com/MeetKai/functionary/tree/main/functionary/train/packing. The code for sequence packing is largely based on it.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
