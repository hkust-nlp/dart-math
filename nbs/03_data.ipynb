{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "> Various (math evaluation) datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dart_math.data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preset Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preset datasets so far:\n",
    "\n",
    "| Dataset                                                                                                                                    | ID                                                       | Size | Stored At                                                                                                                                             | Source                                                                                                                                      |\n",
    "| ------------------------------------------------------------------------------------------------------------------------------------------ | -------------------------------------------------------- | ---: | ----------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| [MATH](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html)/Test      | `\"math/test\"`                                            | 5000 | ðŸ¤— [HuggingFace](https://huggingface.co/datasets/hendrycks/competition_math)                                                                          | ðŸ¤— [hendrycks/competition_math](https://huggingface.co/datasets/hendrycks/competition_math)                                                 |\n",
    "| [MATH](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html)/Train     | `\"math/train\"`                                           | 7500 | ðŸ¤— [HuggingFace](https://huggingface.co/datasets/hendrycks/competition_math)                                                                          | ðŸ¤— [hendrycks/competition_math](https://huggingface.co/datasets/hendrycks/competition_math)                                                 |\n",
    "| [GSM8K](https://arxiv.org/abs/2110.14168)/Test                                                                                             | `\"gsm8k/test\"`                                           | 1319 | ðŸ¤— [HuggingFace](https://huggingface.co/datasets/openai/gsm8k)                                                                                        | ðŸ¤— [gsm8k](https://huggingface.co/datasets/openai/gsm8k)                                                                                    |\n",
    "| [GSM8K(Fixed)](https://huggingface.co/datasets/hkust-nlp/gsm8k-fix)/Train<br>(DEPRECATED: [GSM8K](https://arxiv.org/abs/2110.14168)/Train) | `\"gsm8k-fix/train\"`<br>(DEPRECATED: `\"gsm8k/train\"`)<br> | 7473 | ðŸ¤— [HuggingFace](https://huggingface.co/datasets/hkust-nlp/gsm8k-fix)<br>(DEPRECATED: ðŸ¤— [HuggingFace](https://huggingface.co/datasets/openai/gsm8k)) | ðŸ¤— [gsm8k](https://huggingface.co/datasets/openai/gsm8k)                                                                                    |\n",
    "| [MWPBench](https://github.com/microsoft/unilm/tree/master/mathscale/MWPBench)/CollegeMath/Test                                             | `\"mwpbench/college-math/test\"`                           | 2818 | ðŸŽ¯ [dart/data/dsets](https://github.com/hkust-nlp/dart-math/blob/main/data/dsets/mwpbench/college-math-test.jsonl)                                    | ðŸ± [microsoft/unilm/mathscale/MWPBench](https://github.com/microsoft/unilm/blob/master/mathscale/MWPBench/data/full_train.json)             |\n",
    "| [MWPBench](https://github.com/microsoft/unilm/tree/master/mathscale/MWPBench)/CollegeMath/Train                                            | `\"mwpbench/college-math/train\"`                          | 1281 | ðŸŽ¯ [dart/data/dsets](https://github.com/hkust-nlp/dart-math/blob/main/data/dsets/mwpbench/college-math-train.jsonl)                                   | ðŸ± [microsoft/unilm/mathscale/MWPBench](https://github.com/microsoft/unilm/blob/master/mathscale/MWPBench/data/full_test.json)              |\n",
    "| [MWPBench](https://github.com/microsoft/unilm/tree/master/mathscale/MWPBench)/GaokaoBench                                                  | `\"mwpbench/gaokaobench\"`                                 |  508 | ðŸŽ¯ [dart/data/dsets](https://github.com/hkust-nlp/dart-math/blob/main/data/dsets/mwpbench/gaokaobench.jsonl)                                          | ðŸ± [microsoft/unilm/mathscale/MWPBench](https://github.com/microsoft/unilm/blob/master/mathscale/MWPBench/data/full_test.json)              |\n",
    "| [MWPBench](https://github.com/microsoft/unilm/tree/master/mathscale/MWPBench)/FreshGaokaoMath2023                                          | `\"mwpbench/fresh-gaokao-math-2023\"`                      |   30 | ðŸŽ¯ [dart/data/dsets](https://github.com/hkust-nlp/dart-math/blob/main/data/dsets/mwpbench/fresh-gaokao-math-2023.jsonl)                               | ðŸ± [microsoft/unilm/mathscale/MWPBench](https://github.com/microsoft/unilm/blob/master/mathscale/MWPBench/data/fresh_gaokao_math_2023.json) |\n",
    "| [DeepMind Mathematics](https://openreview.net/forum?id=H1gR5iR5FX)                                                                         | `\"deepmind-mathematics\"`                                 | 1000 | ðŸŽ¯ [dart/data/dsets](https://github.com/hkust-nlp/dart-math/blob/main/data/dsets/deepmind-mathematics.json)                                           | ðŸ± [google-deepmind/mathematics_dataset](https://github.com/google-deepmind/mathematics_dataset)                                            |\n",
    "| [OlympiadBench-Math](https://arxiv.org/abs/2402.14008)                                                                                     | `\"olympiadbench/OE_TO_maths_en_COMP\"`                    |  675 | ðŸŽ¯ [dart/data/dsets](https://github.com/hkust-nlp/dart-math/blob/main/data/dsets/olympiadbench/OE_TO_maths_en_COMP.json)                              | ðŸ± [OpenBMB/OlympiadBench](https://github.com/OpenBMB/OlympiadBench)                                                                        |\n",
    "| [TheoremQA](https://aclanthology.org/2023.emnlp-main.489/)                                                                                 | `\"theoremqa\"`                                            |  800 | ðŸŽ¯ [dart/data/dsets](https://github.com/hkust-nlp/dart-math/blob/main/data/dsets/theoremqa.json)                                                      | ðŸ± [TIGER-AI-Lab/TheoremQA](https://github.com/TIGER-AI-Lab/TheoremQA/blob/main/theoremqa_test.json)                                        |\n",
    "| [Odyssey-Math](https://github.com/protagolabs/odyssey-math/tree/main)                                                                      | `\"odyssey-math\"`                                         |  386 | ðŸŽ¯ [dart/data/dsets](https://github.com/hkust-nlp/dart-math/blob/main/data/dsets/odyssey-math.jsonl)                                                  | ðŸ± [protagolabs/odyssey-math](https://github.com/protagolabs/odyssey-math/blob/main/final-odyssey-math-with-levels.jsonl)                   |\n",
    "| [AOPS](https://artofproblemsolving.com/wiki/index.php)                                                                                     | `\"aops\"`                                                 | 3886 | ðŸŽ¯ [dart/data/dsets](https://github.com/hkust-nlp/dart-math/blob/main/data/dsets/aops.jsonl)                                                          | ðŸŒ [AOPS](https://artofproblemsolving.com/wiki/index.php)                                                                                   |\n",
    "\n",
    "For other datasets, please refer to `load_query_dps` to add by yourself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### load_query_dps\n",
       "\n",
       ">      load_query_dps (dataset:str|list[str]='math-test',\n",
       ">                      max_n_trials:int|list[int]=1,\n",
       ">                      min_n_corrects:int|list[int]=0,\n",
       ">                      prompt_template:str='alpaca')\n",
       "\n",
       "*Load `dataset`(s) as `QueryDataPoint`s.\n",
       "If needed, please add `dataset`s here following the format of the existing datasets,\n",
       "or specify the dataset `.json` path with the stem name as dataset ID.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset | str \\| list[str] | math-test | (List of) dataset ID<br>or path to dataset of samples with \"query\" and \"ref_ans\" fields.<br>Path will not use other two arguments. |\n",
       "| max_n_trials | int \\| list[int] | 1 | (List of) maximum number of raw responses to be generated for each dataset.<br>Non-positive value or `None` means no limit. |\n",
       "| min_n_corrects | int \\| list[int] | 0 | (List of) minimum number of correct responses to be generated for each dataset.<br>Non-positive value or `None` means no limit. |\n",
       "| prompt_template | str | alpaca | ID / Path of the prompt template. |\n",
       "| **Returns** | **list** |  | **`QueryDataPoint` to be input to `dart.gen.gen`.** |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### load_query_dps\n",
       "\n",
       ">      load_query_dps (dataset:str|list[str]='math-test',\n",
       ">                      max_n_trials:int|list[int]=1,\n",
       ">                      min_n_corrects:int|list[int]=0,\n",
       ">                      prompt_template:str='alpaca')\n",
       "\n",
       "*Load `dataset`(s) as `QueryDataPoint`s.\n",
       "If needed, please add `dataset`s here following the format of the existing datasets,\n",
       "or specify the dataset `.json` path with the stem name as dataset ID.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset | str \\| list[str] | math-test | (List of) dataset ID<br>or path to dataset of samples with \"query\" and \"ref_ans\" fields.<br>Path will not use other two arguments. |\n",
       "| max_n_trials | int \\| list[int] | 1 | (List of) maximum number of raw responses to be generated for each dataset.<br>Non-positive value or `None` means no limit. |\n",
       "| min_n_corrects | int \\| list[int] | 0 | (List of) minimum number of correct responses to be generated for each dataset.<br>Non-positive value or `None` means no limit. |\n",
       "| prompt_template | str | alpaca | ID / Path of the prompt template. |\n",
       "| **Returns** | **list** |  | **`QueryDataPoint` to be input to `dart.gen.gen`.** |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(load_query_dps, title_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unified Data Templates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We unify the data format across `dart`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### QueryDataPoint\n",
       "\n",
       ">      QueryDataPoint (dataset:str, query:str, ref_ans:str,\n",
       ">                      prompt_template:dart_math.utils.PromptTemplate='alpaca',\n",
       ">                      n_shots:int=-1, n_trials:int=0, n_corrects:int=0,\n",
       ">                      max_n_trials:int|None=None, min_n_corrects:int|None=None,\n",
       ">                      **kwargs:dict[str,typing.Any])\n",
       "\n",
       "*The query-level data point to generate responses with `vllm` using `sampling_params` (and evaluate with `evaluator`) on.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset | str |  | The dataset name the the query belongs to. E.g. \"math\". |\n",
       "| query | str |  | Raw query, without other prompt. |\n",
       "| ref_ans | str |  | The short reference answer to the `query`. |\n",
       "| prompt_template | PromptTemplate | alpaca | The prompt template object to use. |\n",
       "| n_shots | int | -1 | Number of examples in the few-shot prompt. Negative means adaptive to the datasets. |\n",
       "| n_trials | int | 0 | Number of **raw** responses already generated for the `query`. |\n",
       "| n_corrects | int | 0 | Number of **correct** responses already generated for the `query`. |\n",
       "| max_n_trials | int \\| None | None | Maximum number of trials to generate a response, by default None<br>`None` or Negative means no limit. |\n",
       "| min_n_corrects | int \\| None | None | Maximum number of trials to generate a response, by default None<br>`None` or Negative means no limit. |\n",
       "| kwargs | dict |  | Other fields to store. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### QueryDataPoint\n",
       "\n",
       ">      QueryDataPoint (dataset:str, query:str, ref_ans:str,\n",
       ">                      prompt_template:dart_math.utils.PromptTemplate='alpaca',\n",
       ">                      n_shots:int=-1, n_trials:int=0, n_corrects:int=0,\n",
       ">                      max_n_trials:int|None=None, min_n_corrects:int|None=None,\n",
       ">                      **kwargs:dict[str,typing.Any])\n",
       "\n",
       "*The query-level data point to generate responses with `vllm` using `sampling_params` (and evaluate with `evaluator`) on.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset | str |  | The dataset name the the query belongs to. E.g. \"math\". |\n",
       "| query | str |  | Raw query, without other prompt. |\n",
       "| ref_ans | str |  | The short reference answer to the `query`. |\n",
       "| prompt_template | PromptTemplate | alpaca | The prompt template object to use. |\n",
       "| n_shots | int | -1 | Number of examples in the few-shot prompt. Negative means adaptive to the datasets. |\n",
       "| n_trials | int | 0 | Number of **raw** responses already generated for the `query`. |\n",
       "| n_corrects | int | 0 | Number of **correct** responses already generated for the `query`. |\n",
       "| max_n_trials | int \\| None | None | Maximum number of trials to generate a response, by default None<br>`None` or Negative means no limit. |\n",
       "| min_n_corrects | int \\| None | None | Maximum number of trials to generate a response, by default None<br>`None` or Negative means no limit. |\n",
       "| kwargs | dict |  | Other fields to store. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(QueryDataPoint, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RespSampleBase\n",
       "\n",
       ">      RespSampleBase (dataset:str, query:str, ref_ans:str, resp:str,\n",
       ">                      ans:str=None, correct:bool=None)\n",
       "\n",
       "*The response-level data point containing the query-level data point and other response-level information.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset | str |  | The dataset name the the query belongs to. |\n",
       "| query | str |  | The input query to generate responses on. |\n",
       "| ref_ans | str |  | The reference answer to the query. |\n",
       "| resp | str |  | The generated response. |\n",
       "| ans | str | None | The answer in the generated response, by default None |\n",
       "| correct | bool | None | Whether the generated response is correct, by default None |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RespSampleBase\n",
       "\n",
       ">      RespSampleBase (dataset:str, query:str, ref_ans:str, resp:str,\n",
       ">                      ans:str=None, correct:bool=None)\n",
       "\n",
       "*The response-level data point containing the query-level data point and other response-level information.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset | str |  | The dataset name the the query belongs to. |\n",
       "| query | str |  | The input query to generate responses on. |\n",
       "| ref_ans | str |  | The reference answer to the query. |\n",
       "| resp | str |  | The generated response. |\n",
       "| ans | str | None | The answer in the generated response, by default None |\n",
       "| correct | bool | None | Whether the generated response is correct, by default None |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RespSampleBase, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RespSampleVLLM\n",
       "\n",
       ">      RespSampleVLLM (dataset:str, query:str, ref_ans:str, abs_tol:float=None,\n",
       ">                      resp:str=None, finish_reason:str=None,\n",
       ">                      stop_reason:str=None, cumulative_logprob:float=None,\n",
       ">                      ans:str=None, correct:bool=None, **kwargs)\n",
       "\n",
       "*The response-level data point from `vllm` model, containg extra fields like `finish_reason`, `stop_reason`, `cumulative_logprob`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset | str |  | The dataset name the the query belongs to. |\n",
       "| query | str |  | The input query to generate responses on. |\n",
       "| ref_ans | str |  | The reference answer to the query. |\n",
       "| abs_tol | float | None | The absolute tolerance of the answer. |\n",
       "| resp | str | None | The generated response. |\n",
       "| finish_reason | str | None | The reason for finishing the generation from `vllm` |\n",
       "| stop_reason | str | None | The reason for stopping the generation from `vllm`, e.g. EoS token. |\n",
       "| cumulative_logprob | float | None | The cumulative log probability of the generated response. |\n",
       "| ans | str | None | The generated response. |\n",
       "| correct | bool | None | Whether the generated response is correct. |\n",
       "| kwargs |  |  | Other fields to store. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/hkust-nlp/dart-math/blob/main/dart_math/data.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### RespSampleVLLM\n",
       "\n",
       ">      RespSampleVLLM (dataset:str, query:str, ref_ans:str, abs_tol:float=None,\n",
       ">                      resp:str=None, finish_reason:str=None,\n",
       ">                      stop_reason:str=None, cumulative_logprob:float=None,\n",
       ">                      ans:str=None, correct:bool=None, **kwargs)\n",
       "\n",
       "*The response-level data point from `vllm` model, containg extra fields like `finish_reason`, `stop_reason`, `cumulative_logprob`.*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| dataset | str |  | The dataset name the the query belongs to. |\n",
       "| query | str |  | The input query to generate responses on. |\n",
       "| ref_ans | str |  | The reference answer to the query. |\n",
       "| abs_tol | float | None | The absolute tolerance of the answer. |\n",
       "| resp | str | None | The generated response. |\n",
       "| finish_reason | str | None | The reason for finishing the generation from `vllm` |\n",
       "| stop_reason | str | None | The reason for stopping the generation from `vllm`, e.g. EoS token. |\n",
       "| cumulative_logprob | float | None | The cumulative log probability of the generated response. |\n",
       "| ans | str | None | The generated response. |\n",
       "| correct | bool | None | Whether the generated response is correct. |\n",
       "| kwargs |  |  | Other fields to store. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_doc(RespSampleVLLM, title_level=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
