{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØDART-Math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìù [Paper](https://tongyx361.github.io/assets/dart-math/paper-dart-math.pdf)¬†|¬†ü§ó [Datasets & Models](https://huggingface.co/collections/hkust-nlp/dart-math-665704599b35de59f8fdf6c1) | üê± [Code](https://github.com/hkust-nlp/dart-math) | üê¶ [X (Twitter)](TODO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "<img src=\"https://tongyx361.github.io/assets/dart-math/main-results.png\" alt=\"Main results averaged on 2 in-domain and 4 challenging out-of-domain mathematical reasoning benchmarks.\" height=300px>\n",
    "<img src=\"https://tongyx361.github.io/assets/dart-math/main-nresp-vs-query.png\" alt=\"Number of responses v.s. query descending in difficulty in DART-Math datasets and similar-sized VRT baseline\" height=300px>\n",
    "</div>\n",
    "\n",
    "<div align=\"left\">\n",
    "<sup>\n",
    "Figure 1: <strong>Left:</strong> Average accuracy on 6 mathematical\n",
    "benchmarks. We compare with models fine-tuned on the best, public\n",
    "instruction tuning datasets for mathematical problem-solving: MetaMath\n",
    "<a href=\"https://openreview.net/forum?id=N8N0hgNDRt\">(Yu et al.,\n",
    "2024)</a> with 395K examples, MMIQC\n",
    "<a href=\"https://arxiv.org/abs/2401.09003\">(Liu et al., 2024a)</a> with\n",
    "2.3 million examples, as well as vanilla rejection tuning (VRT) with\n",
    "590K examples. Both <em>DART-Math (Uniform)</em> and <em>DART-Math\n",
    "(Prop2Diff)</em> use 590K training examples. <strong>Right:</strong>\n",
    "Number of responses for each query descending by difficulty across 3\n",
    "synthesis strategies. Queries are from the MATH training split\n",
    "<a href=\"https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html\">(Hendrycks\n",
    "et al., 2021)</a>. VRT is the baseline biased towards easy queries,\n",
    "while <em>Uniform</em> and <em>Prop2Diff</em> are proposed in this work\n",
    "to balance and bias towards difficult queries respectively. Points are\n",
    "slightly shifted and downsampled for clarity.\n",
    "</sup>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Dataset                | Method         | # of Samples |                                     Download                                     |\n",
    "| :--------------------- | :------------- | -----------: | :------------------------------------------------------------------------------: |\n",
    "| `DART-Math-Uniform`    | DARS-Unifrom   |         591k |  ü§ó [HuggingFace](https://huggingface.co/datasets/hkust-nlp/dart-math-uniform)   |\n",
    "| `DART-Math-Hard`       | DARS-Prop2Diff |         585k |    ü§ó [HuggingFace](https://huggingface.co/datasets/hkust-nlp/dart-math-hard)    |\n",
    "| `DART-Math-Pool-MATH`  | DARS-Unifrom   |        1615k | ü§ó [HuggingFace](https://huggingface.co/datasets/hkust-nlp/dart-math-pool-math)  |\n",
    "| `DART-Math-Pool-GSM8K` | DARS-Unifrom   |        2739k | ü§ó [HuggingFace](https://huggingface.co/datasets/hkust-nlp/dart-math-pool-gsm8k) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model                               | [MATH](https://huggingface.co/datasets/hendrycks/competition_math) | [GSM8K](https://huggingface.co/datasets/gsm8k) | [CollegeMath](https://github.com/hkust-nlp/dart-math/tree/main/data/dsets/mwpbench/college-math-test.jsonl) |                                     Download                                      |\n",
    "| :---------------------------------- | -----------------------------------------------------------------: | ---------------------------------------------: | ----------------------------------------------------------------------------------------------------------: | :-------------------------------------------------------------------------------: |\n",
    "| `DART-Math-Llama-3-70B` (Uniform)   |                                                               54.9 |                                       **90.4** |                                                                                                    **38.5** |  ü§ó [HuggingFace](https://huggingface.co/hkust-nlp/dart-math-llama3-70b-uniform)  |\n",
    "| `DART-Math-Llama-3-70B` (Prop2Diff) |                                                           **56.1** |                                           89.6 |                                                                                                        37.9 | ü§ó [HuggingFace](https://huggingface.co/hkust-nlp/dart-math-llama3-70b-prop2diff) |\n",
    "| `DART-Math-DSMath-7B` (Uniform)     |                                                               52.9 |                                       **88.2** |                                                                                                        40.1 |  ü§ó [HuggingFace](https://huggingface.co/hkust-nlp/dart-math-dsmath-7b-uniform)   |\n",
    "| `DART-Math-DSMath-7B` (Prop2Diff)   |                                                           **53.6** |                                           86.8 |                                                                                                    **40.7** | ü§ó [HuggingFace](https://huggingface.co/hkust-nlp/dart-math-dsmath-7b-prop2diff)  |\n",
    "| `DART-Math-Mistral-7B` (Uniform)    |                                                               43.5 |                                       **82.6** |                                                                                                        26.9 |  ü§ó [HuggingFace](https://huggingface.co/hkust-nlp/dart-math-mistral-7b-uniform)  |\n",
    "| `DART-Math-Mistral-7B` (Prop2Diff)  |                                                           **45.5** |                                           81.1 |                                                                                                    **29.4** | ü§ó [HuggingFace](https://huggingface.co/hkust-nlp/dart-math-mistral-7b-prop2diff) |\n",
    "| `DART-Math-Llama-3-8B` (Uniform)    |                                                               45.3 |                                       **82.5** |                                                                                                        27.1 |  ü§ó [HuggingFace](https://huggingface.co/hkust-nlp/dart-math-llama3-8b-uniform)   |\n",
    "| `DART-Math-Llama-3-8B` (Prop2Diff)  |                                                           **46.6** |                                           81.1 |                                                                                                    **28.8** | ü§ó [HuggingFace](https://huggingface.co/hkust-nlp/dart-math-llama3-8b-prop2diff)  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DART-Math` Models: SOTA on Various In-Domain and Out-of-Domain Benchmarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DART-Math` models achieve performance **superior or competitive to previous SOTAs** on 2 in-domain and 4 challenging out-of-domain mathematical reasoning benchmarks, despite using **much\n",
    "smaller datasets** and **no proprietary model like GPT-4**.\n",
    "\n",
    "| Model                                                                                      | [MATH](https://huggingface.co/datasets/hendrycks/competition_math) | [GSM8K](https://huggingface.co/datasets/gsm8k) | [College](https://github.com/hkust-nlp/dart-math/tree/main/data/eval-dsets/mwpbench/college-math-test.jsonl) | [DM](https://github.com/hkust-nlp/dart-math/tree/main/data/eval-dsets/deepmind-mathematics.json) | [Olympiad](https://github.com/hkust-nlp/dart-math/tree/main/data/eval-dsets/olympiadbench/OE_TO_maths_en_COMP.json) | [Theorem](https://github.com/hkust-nlp/dart-math/tree/main/data/eval-dsets/theoremqa.json) |      AVG |\n",
    "| :----------------------------------------------------------------------------------------- | -----------------------------------------------------------------: | ---------------------------------------------: | -----------------------------------------------------------------------------------------------------------: | -----------------------------------------------------------------------------------------------: | ------------------------------------------------------------------------------------------------------------------: | -----------------------------------------------------------------------------------------: | -------: |\n",
    "| GPT-4 (0314)                                                                               |                           [52.6](https://arxiv.org/abs/2403.04706) |       [94.7](https://arxiv.org/abs/2403.04706) |                                                                     [24.4](https://arxiv.org/abs/2403.02884) |                                                                                               -- |                                                                                                                  -- |                                                                                         -- |       -- |\n",
    "| Llama-3-70B-MetaMath                                                                       |                                                               44.9 |                                           88.0 |                                                                                                         31.9 |                                                                                             53.2 |                                                                                                                11.6 |                                                                                       21.9 |     41.9 |\n",
    "| [`DART-Math-Llama-3-70B`](https://huggingface.co/hkust-nlp/dart-math-llama3-70b-prop2diff) |                                                           **56.1** |                                       **89.6** |                                                                                                     **37.9** |                                                                                         **64.1** |                                                                                                            **20.0** |                                                                                   **28.2** | **49.3** |\n",
    "| DeepSeekMath-7B-MetaMath                                                                   |                                                               43.7 |                                           81.8 |                                                                                                         33.7 |                                                                                             53.0 |                                                                                                                13.6 |                                                                                       23.2 |     41.5 |\n",
    "| [DeepSeekMath-7B-RL](https://huggingface.co/deepseek-ai/deepseek-math-7b-rl)               |                                                               53.1 |                                           88.4 |                                                                                                         41.3 |                                                                                             58.3 |                                                                                                                18.7 |                                                                                       35.9 |     49.3 |\n",
    "| [`DART-Math-DSMath-7B`](https://huggingface.co/hkust-nlp/dart-math-dsmath-7b-prop2diff)    |                                                           **53.6** |                                       **86.8** |                                                                                                     **40.7** |                                                                                         **61.6** |                                                                                                            **21.7** |                                                                                   **32.2** | **49.4** |\n",
    "| Mistral-7B-MetaMath                                                                        |                                                               29.8 |                                           76.5 |                                                                                                         19.3 |                                                                                             28.0 |                                                                                                                 5.9 |                                                                                       14.0 |     28.9 |\n",
    "| [`DART-Math-Mistral-7B`](https://huggingface.co/hkust-nlp/dart-math-mistral-7b-prop2diff)  |                                                           **45.5** |                                       **81.1** |                                                                                                     **29.4** |                                                                                         **45.1** |                                                                                                            **14.7** |                                                                                   **17.0** | **38.8** |\n",
    "| Llama-3-8B-MetaMath                                                                        |                                                               32.5 |                                           77.3 |                                                                                                         20.6 |                                                                                             35.0 |                                                                                                                 5.5 |                                                                                       13.8 |     30.8 |\n",
    "| [`DART-Math-Llama-3-8B`](https://huggingface.co/hkust-nlp/dart-math-llama3-8b-prop2diff)   |                                                           **46.6** |                                       **81.1** |                                                                                                     **28.8** |                                                                                         **48.0** |                                                                                                            **14.5** |                                                                                   **19.4** | **39.7** |\n",
    "\n",
    "<sup>**Abbreviations**: College (CollegeMath), DM (DeepMind Mathematics), Olympiad (OlympiadBench-Math), Theorem (TheoremQA). **Bold** means the best score by SFT on the respective base model here. `DART-Math` models here are fine-tuned on the [`DART-Math-Hard` dataset](https://huggingface.co/datasets/hkust-nlp/dart-math-hard).</sup>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DART-Math` Datasets: SOTA & Data-Efficient & Open-Source\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DART-Math` are the **state-of-the-art** and **data-efficient** **open-source** instruction tuning datasets for mathematical reasoning.\n",
    "\n",
    "Most of previous datasets are **constructed with ChatGPT**, and many of them are **not open-source**, especially for ones of the best performance.\n",
    "\n",
    "| Math SFT Dataset                                                                   | # of Samples (k) | Synthesis Agent(s)  |                                 Open-Source                                 |\n",
    "| :--------------------------------------------------------------------------------- | ---------------: | :------------------ | :-------------------------------------------------------------------------: |\n",
    "| [WizardMath](https://arxiv.org/abs/2308.09583)                                     |               96 | GPT-4               |                                      ‚úó                                      |\n",
    "| [MetaMathQA](https://arxiv.org/abs/2309.12284)                                     |              395 | GPT-3.5             |          [‚úì](https://huggingface.co/datasets/meta-math/MetaMathQA)          |\n",
    "| [MMIQC](https://arxiv.org/abs/2401.09003)                                          |             2294 | GPT-4+GPT-3.5+Human |             [‚úì](https://huggingface.co/datasets/Vivacem/MMIQC)              |\n",
    "| [Orca-Math](https://arxiv.org/abs/2402.14830)                                      |              200 | GPT-4               | [‚úì](https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k) |\n",
    "| [Xwin-Math-V1.1](https://arxiv.org/abs/2403.04706)                                 |             1440 | GPT-4               |                                      ‚úó                                      |\n",
    "| [KPMath-Plus](https://arxiv.org/abs/2403.02333)                                    |             1576 | GPT-4               |                                      ‚úó                                      |\n",
    "| [MathScaleQA](https://arxiv.org/abs/2403.02884)                                    |             2021 | GPT-3.5+Human       |                                      ‚úó                                      |\n",
    "| [`DART-Math-Uniform`](https://huggingface.co/datasets/hkust-nlp/dart-math-uniform) |              591 | DeepSeekMath-7B-RL  |      [‚úì](https://huggingface.co/datasets/hkust-nlp/dart-math-uniform)       |\n",
    "| [`DART-Math-Hard`](https://huggingface.co/datasets/hkust-nlp/dart-math-hard)       |              585 | DeepSeekMath-7B-RL  |        [‚úì](https://huggingface.co/datasets/hkust-nlp/dart-math-hard)        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `DARS` -- Difficulty-Aware Rejection Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our analysis of previous datasets reveals **severe biases towards easy queries**, with **frequent failures to generate any correct response for the most challenging queries**.\n",
    "\n",
    "This primarily arises from their constuction method, **vanilla rejection sampling**, where **the same number** of responses are sampled for each query, yet the likelihood of obtaining correct responses for difficult queries is significantly lower, sometimes even zero.\n",
    "\n",
    "Motivated by the observation above and the intuitive that difficult samples are critical for learning complexing reasoning, we propose **Difficulty-Aware Rejection Sampling** (`DARS`) to eliminate the bias towards easy queries.\n",
    "Specifically, we introduce two strategies to increase the number of correct responses for difficult queries:\n",
    "\n",
    "1. **Uniform**, which involves sampling responses for each query until **each query accumulates $k_u$ correct\n",
    "   responses**, where $k_u$ is a preset hyperparameter determined by the desired size of the synthetic dataset;\n",
    "2. **Prop2Diff**, where we continue sampling responses until the number of correct responses for each\n",
    "   query is **proportional to its difficulty score**. The most challenging queries will receive $k_p$ responses\n",
    "   and kp is a hyperparameter. This method introduces a deliberate bias in the opposite direction to\n",
    "   vanilla rejection sampling, towards more difficult queries, inspired by previous works\n",
    "   that demonstrate **difficult samples can be more effective to enhance model capabilities** ([Sorscher et al.,\n",
    "   2022](https://proceedings.neurips.cc/paper_files/paper/2022/hash/7b75da9b61eda40fa35453ee5d077df6-Abstract-Conference.html); [Liu et al., 2024b](https://openreview.net/forum?id=BTKAeLqLMw)).\n",
    "\n",
    "See [Figure 1 (Right)](https://tongyx361.github.io/assets/dart-math/main-nresp-vs-query.png) for examples of `DART-Math-Uniform` by `DARS-Uniform` and `DART-Math-Hard` by `DARS-Prop2Diff`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Quick Start / Reproduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend using¬†[Conda](https://docs.conda.io/projects/miniconda) and [pip](https://pip.pypa.io/en/stable/#)¬†to manage your environment. Run the following commands to setup your environment:\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/hkust-nlp/dart-math.git && cd dart-math\n",
    "conda create --name dart-math --yes python=3.11\n",
    "conda activate dart-math\n",
    "pip install -r requirements.txt\n",
    "pip install flash-attn --no-build-isolation\n",
    "```\n",
    "\n",
    "For common users/developers, please just run the following command the install the `dart-math` package:\n",
    "\n",
    "```shell\n",
    "pip install -e \".\"\n",
    "```\n",
    "\n",
    "For intended contributors, we recommend installing the package with the `dev` extras:\n",
    "\n",
    "```shell\n",
    "pip install -e \".[dev]\"\n",
    "pre-commit install\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üî® Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement an efficient training pipeline utilizing various techniques. Notably, [**sequence packing**](https://hkust-nlp.github.io/dart-math/train.html#sequence-packing) accelerates training by 6-8x in our setting and possibly more in other settings. (See [how to integrate sequence packing in 4 lines of code](https://hkust-nlp.github.io/dart-math/train.html#accelerating-several-times-with-sequence-packing-in-4-lines-of-code).)\n",
    "\n",
    "Please refer to\n",
    "\n",
    "- the [training Python script](https://github.com/hkust-nlp/dart-math/blob/main/pipeline/train.py) for code of training based on the [HuggingFace `Trainer`](https://huggingface.co/docs/transformers/en/main_classes/trainer) and utilizing [sequence packing](https://hkust-nlp.github.io/dart-math/train.html#sequence-packing).\n",
    "- the [single-node](https://github.com/hkust-nlp/dart-math/blob/main/scripts/train-single-node.sh)/[multi-node](https://github.com/hkust-nlp/dart-math/blob/main/scripts/train-multi-node.sh) training `bash` script for code of training based on [HuggingFace `accelerate`](https://huggingface.co/docs/accelerate/index) and [`deepspeed`](https://www.deepspeed.ai)\n",
    "\n",
    "Here, we provide some example commands as well as reproduction instructions for our work:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Single-Node Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to reproduce training `DART-Math-Llama3-8B-Prop2Diff` on a node of 8 A100 GPUs, please run the following command:\n",
    "\n",
    "```shell\n",
    "bash scripts/train-single-node.sh \\\n",
    "    --data_path \"hkust-nlp/dart-math-hard\" \\\n",
    "    --model_path \"meta-llama/Meta-Llama-3-8B\" \\\n",
    "    --lr \"5e-5\" --bs 64 --n_grad_acc_steps 1 --n_epochs 1 \\\n",
    "    --gpu_ids \"0,1,2,3,4,5,6,7\" \\\n",
    "    --output_dir \"models/dart-math-llama3-8b-prop2diff\"\n",
    "```\n",
    "\n",
    "To reproduce other training settings, just refer to the paper and modify the `--data_path`, `--model_path`, `--lr`, `--n_grad_acc_steps`, `--n_epochs` and `--output_dir` arguments accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Node Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reproduce training `DART-Math-Llama3-70B-Prop2Diff` on 4 nodes of 8 A100 GPUs, please first edit the `cfgs/deepspeed/hostfile` according to your enviroment and then run the following command:\n",
    "\n",
    "```shell\n",
    "bash scripts/train-multi-node.sh \\\n",
    "    --data_path \"hkust-nlp/dart-math-hard\" \\\n",
    "    --model_path \"meta-llama/Meta-Llama-3-70B\" \\\n",
    "    --lr \"2e-5\" --bs 64 --n_grad_acc_steps 1 --n_epochs 1 \\\n",
    "    --n_nodes 4 \\\n",
    "    --output_dir \"models/dart-math-llama3-70b-prop2diff\"\n",
    "```\n",
    "\n",
    "To reproduce training `DART-Math-Llama3-70B-Uniform` on 4 nodes of 8 A100 GPUs, just change `--data_path` to `\"hkust-nlp/dart-math-uniform\"`.\n",
    "\n",
    "<details>\n",
    "<summary>The off-the-shelf command to train `DART-Math-Llama3-70B-Uniform`</summary>\n",
    "```shell\n",
    "bash scripts/train-multi-node.sh \\\n",
    "    --data_path \"hkust-nlp/dart-math-uniform\" \\\n",
    "    --model_path \"meta-llama/Meta-Llama-3-70B\" \\\n",
    "    --lr \"2e-5\" --bs 64 --n_grad_acc_steps 1 --n_epochs 1 \\\n",
    "    --n_nodes 4 \\\n",
    "    --output_dir \"models/dart-math-llama3-70b-prop2diff\"\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öñÔ∏è Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilize [vLLM](https://docs.vllm.ai/en/latest/index.html) to accelerate inference and an elaborate answer extraction and correctness judgement pipeline based on regular expressions and [SymPy](https://www.sympy.org) symbolic calculation, which is able to correctly process\n",
    "\n",
    "- most **mathematical objects** such as matrices (vectors), intervals, symbols besides numbers,\n",
    "- as well as some **special texts** like bool expressions, dates and times.\n",
    "\n",
    "For example, to reproduce one pass of greedy decoding with `DART-Math-Mistral-7B-Prop2Diff` on the 6 benchmarks in Table 2 on GPU 0, please run the following command:\n",
    "\n",
    "```shell\n",
    "CUDA_VISIBLE_DEVICES=\"0\" python pipeline/gen.py \\\n",
    "    --gen_save_path \"data/res/dart-math-mistral-7b-prop2diff.jsonl\" \\\n",
    "    --model_name_or_path \"hkust-nlp/dart-math-mistral-7b-prop2diff\" \\\n",
    "    --datasets \"math-test\" \"gsm8k-test\" \"mwpbench/college-math-test\" \"deepmind-mathematics\" \\\n",
    "        \"olympiadbench/OE_TO_maths_en_COMP\" \"theoremqa\" \\\n",
    "    --max_new_tokens 2048 --temperature 0 --top_p 0.95 \\\n",
    "    --prompt_template \"alpaca\" --n_shots -1 \\\n",
    "    --inf_seed -1 \\\n",
    "    --max_n_trials 1\n",
    "```\n",
    "\n",
    "To reproduce other inference settings, just refer to the paper and modify the `--model_name_or_path` and `--gen_save_path` arguments accordingly.\n",
    "\n",
    "For other general inference settings, please modify the command or directly modify the [script](https://github.com/hkust-nlp/dart-math/blob/main/pipeline/gen.py).\n",
    "\n",
    "You can also add the `--gen_only` option to only generate responses without evaluation and use the `EvaluatorMathBatch` to grade the generations by yourself. Please check the [grading script](pipeline/grade.py) for example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóÇ Data Synthesis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data synthesis pipeline is compatible with the evaluation pipeline, please **modify the `--min_n_corrects` and `--max_n_trials` arguments** to meet your needs.\n",
    "\n",
    "For example, to reproduce the **synthesis of `DART-Math-Uniform`**, amortizing the workload to multiple GPUs, please run the following command:\n",
    "\n",
    "```shell\n",
    "gpu_ids_list=(\"0\" \"1\" \"2\" \"3\" \"4\" \"5\" \"6\" \"7\")\n",
    "min_n_corrects=40\n",
    "min_n_corrects_per_gpu=$((min_n_corrects / ${#gpu_ids_list[@]})) # 5 here\n",
    "\n",
    "mkdir -p logs\n",
    "for gpu_ids in \"${gpu_ids_list[@]}\"; do\n",
    "    exp_name=\"dart-math-uniform-gpu${gpu_ids}\"\n",
    "    CUDA_VISIBLE_DEVICES=\"${gpu_ids}\" python pipeline/gen.py \\\n",
    "        --gen_save_path \"data/res/${exp_name}.jsonl\" \\\n",
    "        --model_name_or_path \"deepseek-ai/deepseek-math-7b-rl\" \\\n",
    "        --datasets \"math-train\" \"gsm8k-train\" \\\n",
    "        --max_new_tokens 2048 --temperature 1.6 --top_p 0.95 \\\n",
    "        --prompt_template \"deepseekmath\" --n_shots 0 \\\n",
    "        --inf_seed -1 \\\n",
    "        --min_n_corrects \"${min_n_corrects_per_gpu}\" --max_n_trials 0 \\\n",
    "        >\"logs/${exp_name}.log\" 2>&1 &\n",
    "    # NOTE: `--max_n_trials 0` means possible infinite trials, kill the job manually when needed\n",
    "done\n",
    "```\n",
    "\n",
    "To reproduce the data synthesis of the **Vanilla Rejection Tuning (VRT) baseline** in the paper, just set `--max_n_trials 52 --min_n_corrects 0`.\n",
    "\n",
    "<details>\n",
    "<summary>The off-the-shelf command to reproduce the data synthesis of the Vanilla Rejection Tuning (VRT) baseline in the paper</summary>\n",
    "```shell\n",
    "CUDA_VISIBLE_DEVICES=\"0\" python pipeline/gen.py \\\n",
    "    --gen_save_path \"data/res/dart-math-uniform.jsonl\" \\\n",
    "    --model_name_or_path \"deepseek-ai/deepseek-math-7b-rl\" \\\n",
    "    --datasets \"math-train\" \"gsm8k-train\" \\\n",
    "    --max_new_tokens 2048 --temperature 1.6 --top_p 0.95 \\\n",
    "    --prompt_template \"cot\" --n_shots 0 \\\n",
    "    --inf_seed -1 \\\n",
    "    --max_n_trials 52 --min_n_corrects 0 # no requirement for correct responses\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "After the synthesis, you can use the [curation script](pipeline/curate.py) to curate the final dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [`dart-math` Package](https://hkust-nlp.github.io/dart-math): Efficient and Flexible Training & Inference & Evaluation Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We package our code of effcient and flexible training & inference & evaluation pipelines into `dart-math` and document it at [this website](https://hkust-nlp.github.io/dart-math/quick-start.html).\n",
    "\n",
    "The `dart-math` package provides the following useful features besides ones mentioned above:\n",
    "\n",
    "- **Tool-integrated reasoning**: reasoning in natural language interleaved with Python code (see the `code_exec_cfg` attribute of `Generator`);\n",
    "- ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowlegements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to:\n",
    "\n",
    "- [`nbdev`](https://nbdev.fast.ai/) for generating the [wonderful documentation website](https://hkust-nlp.github.io/dart-math),\n",
    "- [`stanford_alpaca`](https://github.com/tatsu-lab/stanford_alpaca) for reference code about training,\n",
    "- [`functionary`](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing) for reference code about [sequence packing](https://hkust-nlp.github.io/dart-math/train.html#sequence-packing).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find our data, model or code useful for your work, please kindly cite [our paper](https://tongyx361.github.io/assets/dart-math/paper-dart-math.pdf):\n",
    "\n",
    "```latex\n",
    "@article{tong2024dartmath,\n",
    "  author = {Yuxuan Tong, Xiwen Zhang, Rui Wang, Ruidong Wu, Junxian He},\n",
    "  title = {DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving},\n",
    "  year = {2024},\n",
    "  publisher = {GitHub},\n",
    "  journal = {preprint},\n",
    "  howpublished = {https://tongyx361.github.io/assets/dart-math/paper-dart-math.pdf},\n",
    "}\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
