{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "dev"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze token size of dataset(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_id: str = \"deepseek-ai/deepseek-math-7b-rl\"\n",
    "tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast]\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_token_stats(\n",
    "    dataset: Union[Dataset, str] = \"hkust-nlp/dart-math-hard\",\n",
    "    tokenizer: Union[\n",
    "        PreTrainedTokenizer, PreTrainedTokenizerFast, str\n",
    "    ] = \"deepseek-ai/deepseek-math-7b-rl\",\n",
    ") -> dict:\n",
    "    if isinstance(dataset, str):\n",
    "        dataset = load_dataset(dataset, split=\"train\")\n",
    "    if isinstance(tokenizer, str):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer)\n",
    "\n",
    "    def tokenize_batch(examples) -> dict[str, int]:\n",
    "        query_tokens = tokenizer(examples[\"query\"], truncation=False, padding=False)\n",
    "        response_tokens = tokenizer(\n",
    "            examples[\"response\"], truncation=False, padding=False\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"query_token_size\": [len(tokens) for tokens in query_tokens[\"input_ids\"]],\n",
    "            \"response_token_size\": [\n",
    "                len(tokens) for tokens in response_tokens[\"input_ids\"]\n",
    "            ],\n",
    "        }\n",
    "\n",
    "    # Apply the tokenization to the entire dataset\n",
    "    token_sizes = dataset.map(\n",
    "        tokenize_batch,\n",
    "        batched=True,\n",
    "        batch_size=1024,\n",
    "        num_proc=16,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "\n",
    "    # Calculate average token sizes\n",
    "    avg_query_size = sum(token_sizes[\"query_token_size\"]) / len(\n",
    "        token_sizes[\"query_token_size\"]\n",
    "    )\n",
    "    avg_response_size = sum(token_sizes[\"response_token_size\"]) / len(\n",
    "        token_sizes[\"response_token_size\"]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"average_query_token_size\": avg_query_size,\n",
    "        \"average_response_token_size\": avg_response_size,\n",
    "        \"total_query_tokens\": sum(token_sizes[\"query_token_size\"]),\n",
    "        \"total_response_tokens\": sum(token_sizes[\"response_token_size\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad09ccbd667c4d18bc5c4bc19ac74014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/585392 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'average_query_token_size': 96.58649076174598,\n",
       " 'average_response_token_size': 480.3031387514691,\n",
       " 'total_query_tokens': 56540959,\n",
       " 'total_response_tokens': 281165615}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_token_stats(\"hkust-nlp/dart-math-hard\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5103b65a4bbd4e17b59103742a0ecfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=16):   0%|          | 0/590705 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'average_query_token_size': 68.52043236471674,\n",
       " 'average_response_token_size': 273.8289179878281,\n",
       " 'total_query_tokens': 40475362,\n",
       " 'total_response_tokens': 161752111}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_token_stats(\"hkust-nlp/dart-math-uniform\", tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
