{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "dev"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "\n",
    "> Generate with specified stopping criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dart_math.utils import (\n",
    "    init_logging,\n",
    "    get_pathname_from_name_or_path,\n",
    "    PromptTemplate,\n",
    ")\n",
    "\n",
    "from dart_math.gen import is_dp_dars_finished, Generator\n",
    "from dart_math.eval import EvaluatorMathBatch\n",
    "from dart_math.data import load_query_dps, RespSampleVLLM\n",
    "from dart_math.exec import CodeExecCfg\n",
    "from dart_math.utils import PROJ_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [2024-07-06 22:07:05.22] [pid 3781391] [/ssddata/tongyx/projects/dart-math/dart_math/utils.py:306:init_logging]\n",
      "log_path = None\n"
     ]
    }
   ],
   "source": [
    "init_logging()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"vLLM generation\", allow_abbrev=False)\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--gen_save_path\",\n",
    "    type=str,\n",
    "    default=os.path.join(PROJ_HOME, \"data/res/gen.jsonl\"),\n",
    "    help=\"Path save results of generation (and evaluation).\",\n",
    ")\n",
    "\n",
    "# Device\n",
    "parser.add_argument(\n",
    "    \"--gpu_mem_util\",\n",
    "    type=float,\n",
    "    default=0.85,\n",
    "    help=\"GPU memory utilization for vLLM. Default: 0.85 in case of OOM.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--swap_space\", type=float, default=60, help=\"CPU swap space in GB for vLLM.\"\n",
    ")\n",
    "\n",
    "# Model\n",
    "parser.add_argument(\n",
    "    \"--model_name_or_path\",\n",
    "    type=str,\n",
    "    default=\"deepseek-ai/deepseek-math-7b-rl\",\n",
    "    help=\"HF-style model name or path.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--dtype\",\n",
    "    type=str,\n",
    "    default=\"bfloat16\",\n",
    "    help=\"Data type for the model.\",\n",
    ")\n",
    "\n",
    "# Data\n",
    "parser.add_argument(\n",
    "    \"--datasets\",\n",
    "    type=str,\n",
    "    nargs=\"+\",\n",
    "    default=[\"math-test\"],\n",
    "    help=\"Dataset(s) to generate on.\",\n",
    ")\n",
    "\n",
    "# Generation configurations\n",
    "parser.add_argument(\n",
    "    \"--temperature\",\n",
    "    type=float,\n",
    "    default=0,\n",
    "    help=\"Temperature for sampling.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--top_p\",\n",
    "    type=float,\n",
    "    default=0.95,\n",
    "    help=\"Top-p for sampling.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_new_toks\",\n",
    "    type=int,\n",
    "    default=2048,\n",
    "    help=\"Maximum number of new tokens.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_shots\",\n",
    "    type=int,\n",
    "    default=-1,\n",
    "    help=\"Number of shots for prompting. -1 means adaptive to datasets.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prompt_template\",\n",
    "    type=str,\n",
    "    default=\"cot\",\n",
    "    help=\"ID / Path to the file of prompt template.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_paths\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"Number of generated completions per request. NOTE: might cause bug in vLLM (0.4.2).\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--save_gen_path_bs\",\n",
    "    type=int,\n",
    "    default=2**14,\n",
    "    help=\"# Completions = # Paths per request * # Requests. Values <= 0 mean adaptive.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--inf_seed\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"Random seed for inference. -1 means using us timestamp mod 2^32.\",\n",
    ")\n",
    "\n",
    "# Stopping criteria\n",
    "parser.add_argument(\n",
    "    \"--max_n_trials\",\n",
    "    nargs=\"+\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"(List of) maximum number of trials for each query. Non-positive means no limit.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gen_only\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether to only generate reponses and not evaluate the generated completions.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--min_n_corrects\",\n",
    "    nargs=\"+\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"(List of) minimum number of correct completions per query needed to stop generation. Non-positive means no goal.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--strict_extract\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether to extract answers strictly. If `False`, speculate the answer from the last number if needed.\",\n",
    ")\n",
    "\n",
    "# Code execution\n",
    "parser.add_argument(\n",
    "    \"--code_exec_cfg\",\n",
    "    type=str,\n",
    "    default=\"\",\n",
    "    help=\"ID / Path to file of the code execution configuration.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_n_workers\",\n",
    "    type=int,\n",
    "    default=None,\n",
    "    help=\"The maximum number of CPU core workers to execute the code with multi-processing. Default as `None`, meaning using default value of `code_exec_cfg`. \",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_n_calls\",\n",
    "    type=int,\n",
    "    default=None,\n",
    "    help=\"The maximum number of calls to the code execution function.\\nThis could be large because there is token length limit already.\\nDefault as `None`, meaning using default value of `code_exec_cfg`.  Non-positive values mean no limit.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--trunc_len\",\n",
    "    type=int,\n",
    "    nargs=2,\n",
    "    default=None,\n",
    "    help=\"The maximum lengths to truncate the output into the beginning and end.\\nDefault as `None`, meaning using default value of `code_exec_cfg`. Double non-positive values like `(0, 0)` mean no truncation. \",\n",
    ")\n",
    "\n",
    "args, unk_args = parser.parse_known_args()\n",
    "\n",
    "for arg_str in unk_args:\n",
    "    if arg_str.startswith(\"--f=\"):\n",
    "        continue  # For Jupyter notebook\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown arguments: {unk_args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "dev"
    ]
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "\n",
    "# Test tool-integrated reasoning\n",
    "args.prompt_template = \"tool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.inf_seed == -1:\n",
    "    args.inf_seed = int(time.time() * 10**6) % 2**32\n",
    "    logging.warning(f\"args.inf_seed=-1 -> Setting {args.inf_seed=}\")\n",
    "\n",
    "if \"tool\" in args.prompt_template and args.code_exec_cfg == \"\":\n",
    "    args.code_exec_cfg = \"python\"\n",
    "    logging.warning(f\"{args.prompt_template=} -> Setting {args.code_exec_cfg=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirname = get_pathname_from_name_or_path(args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = (\n",
    "    PromptTemplate.get_prompt_template_from_prompt_type_and_model(\n",
    "        prompt_type=args.prompt_template, model_name_or_path=args.model_name_or_path\n",
    "    )\n",
    "    if args.prompt_template in [\"cot\", \"tool\"]\n",
    "    else PromptTemplate.load_from_id_or_path(args.prompt_template)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssddata/tongyx/miniconda3/envs/dart-math/lib/python3.11/site-packages/datasets/load.py:1486: FutureWarning: The repository for hendrycks/competition_math contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/hendrycks/competition_math\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "[INFO] [2024-06-16 13:15:02.90] [pid 646448] [/tmp/ipykernel_646448/3154498625.py:2:<module>]\n",
      "Loaded 5000 query data points.\n"
     ]
    }
   ],
   "source": [
    "query_dps = load_query_dps(args.datasets, args.max_n_trials, args.min_n_corrects)\n",
    "logging.info(f\"Loaded {len(query_dps)} query data points.\")\n",
    "# TODO: response-wise prompt template\n",
    "for query_dp in query_dps:\n",
    "    query_dp.prompt_template = prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] [2024-06-16 13:14:58.450] [pid 646448] [/tmp/ipykernel_646448/3821527518.py:5:<module>]\n",
      "Temperature is too small. Setting temperautre = 0, n_paths = 1, top_p = 1 for vLLM.\n"
     ]
    }
   ],
   "source": [
    "if args.temperature <= 1e-5:\n",
    "    args.temperature = 0\n",
    "    args.n_paths = 1\n",
    "    args.top_p = 1\n",
    "    logging.warning(\n",
    "        f\"args.temperature<=1e-5 -> Setting {args.temperature=}, {args.n_paths=}, {args.top_p=} for vLLM.\"\n",
    "    )\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    n=args.n_paths,\n",
    "    temperature=args.temperature,\n",
    "    top_p=args.top_p,\n",
    "    max_tokens=args.max_new_toks,\n",
    "    skip_special_tokens=True,\n",
    "    seed=args.inf_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [2024-06-16 13:15:02.123] [pid 646448] [/tmp/ipykernel_646448/1135370137.py:5:<module>]\n",
      "sampling_params = SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=0, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=['User:', 'Assistant:'], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=2048, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None)\n"
     ]
    }
   ],
   "source": [
    "sampling_params.stop = [\n",
    "    prompt_template.query_prompt.strip(),\n",
    "    prompt_template.resp_prompt.strip(),\n",
    "]\n",
    "logging.info(f\"sampling_params = {sampling_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssddata/tongyx/miniconda3/envs/dart-math/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-16 13:15:02 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='deepseek-ai/deepseek-math-7b-rl', speculative_config=None, tokenizer='deepseek-ai/deepseek-math-7b-rl', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=deepseek-ai/deepseek-math-7b-rl)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-16 13:15:04 weight_utils.py:207] Using model weights format ['*.safetensors']\n",
      "INFO 06-16 13:15:08 model_runner.py:146] Loading model weights took 12.8725 GB\n",
      "INFO 06-16 13:15:08 gpu_executor.py:83] # GPU blocks: 7255, # CPU blocks: 8192\n",
      "INFO 06-16 13:16:06 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-16 13:16:06 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-16 13:16:11 model_runner.py:924] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [2024-06-16 13:16:11.868] [pid 646448] [/tmp/ipykernel_646448/4134619846.py:11:<module>]\n",
      "LLM loaded!\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=args.model_name_or_path,\n",
    "    tokenizer=args.model_name_or_path,\n",
    "    tensor_parallel_size=torch.cuda.device_count(),\n",
    "    dtype=args.dtype,\n",
    "    seed=args.inf_seed,\n",
    "    gpu_memory_utilization=args.gpu_mem_util,\n",
    "    swap_space=args.swap_space,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "logging.info(\"LLM loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_exec_cfg = (\n",
    "    CodeExecCfg.load_from_id_or_path(args.code_exec_cfg) if args.code_exec_cfg else None\n",
    ")\n",
    "if code_exec_cfg:\n",
    "    if args.max_n_workers is not None:\n",
    "        code_exec_cfg.max_n_workers = args.max_n_workers\n",
    "    if args.max_n_calls is not None:\n",
    "        code_exec_cfg.max_n_calls = args.max_n_calls\n",
    "    if args.trunc_len is not None:\n",
    "        code_exec_cfg.trunc_len = args.trunc_len\n",
    "\n",
    "    print(f\"{code_exec_cfg.__dict__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [2024-06-16 13:16:12.140] [pid 646448] [/ssddata/tongyx/projects/dart-math/dart_math/gen.py:151:gen_pure]\n",
      "sampling_params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=0, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=['User:', 'Assistant:', '```output'], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=2048, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None)\n",
      "[INFO] [2024-06-16 13:16:12.141] [pid 646448] [/ssddata/tongyx/projects/dart-math/dart_math/gen.py:152:gen_pure]\n",
      "input_strs[0]: User: How many vertical asymptotes does the graph of $y=\\frac{2}{x^2+x-6}$ have?\n",
      "Please integrate natural language reasoning with programs to solve the problem above, and put your final answer within \\boxed{}.\n",
      "\n",
      "Assistant:\n",
      "Processed prompts:  37%|███▋      | 1842/5000 [02:10<07:28,  7.04it/s, Generation Speed: 2945.22 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-16 13:18:24 scheduler.py:1077] Sequence group 2097 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 5000/5000 [06:17<00:00, 13.26it/s, Generation Speed: 3101.52 toks/s]\n",
      "[INFO] [2024-06-16 13:22:31.317] [pid 646448] [/ssddata/tongyx/projects/dart-math/dart_math/gen.py:210:gen_pure]\n",
      "len(remain_ids): 4978\n",
      "[INFO] [2024-06-16 13:22:31.317] [pid 646448] [/ssddata/tongyx/projects/dart-math/dart_math/gen.py:215:gen_pure]\n",
      "cells_list: (#4978)[['from sympy import symbols, solveset, S\\n\\ndef count_vertical_asymptotes():\\n    \"\"\"How many vertical asymptotes does the graph of $y=\\\\frac{2}{x^2+x-6}$ have?\"\"\"\\n    x = symbols(\\'x\\')\\n    denominator = x**2 + x - 6\\n    asymptotes = solveset(denominator, x, domain=S.Reals)\\n    num_asymptotes = len(asymptotes)\\n\\n    return num_asymptotes\\n\\nresult = count_vertical_asymptotes()\\nprint(result)'],...]\n",
      "Executing:  31%|███       | 1530/4978 [04:24<09:55,  5.79it/s]  \n"
     ]
    }
   ],
   "source": [
    "generator = Generator(\n",
    "    llm,\n",
    "    sampling_params,\n",
    "    resp_sample_cls=RespSampleVLLM,\n",
    "    batch_evaluator=(\n",
    "        EvaluatorMathBatch(strict_extract=args.strict_extract)\n",
    "        if not args.gen_only\n",
    "        else None\n",
    "    ),\n",
    "    code_exec_cfg=code_exec_cfg,\n",
    ")\n",
    "generator.gen(\n",
    "    query_dps=query_dps,\n",
    "    dp_stop_criteria=is_dp_dars_finished,\n",
    "    save_path=args.gen_save_path,\n",
    "    n_paths_per_save=args.save_gen_path_bs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [2024-06-16 11:52:25.783] [pid 587771] [/tmp/ipykernel_587771/3111312346.py:1:<module>]\n",
      "Generation done!\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Generation done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
