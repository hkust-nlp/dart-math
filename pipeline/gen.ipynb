{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "\n",
    "> Generate with specified stopping criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dart_math.utils import (\n",
    "    init_logging,\n",
    "    get_pathname_from_name_or_path,\n",
    "    PromptTemplate,\n",
    ")\n",
    "\n",
    "from dart_math.gen import gen, get_prompt_template4model, is_dp_dars_finished\n",
    "from dart_math.eval import EvaluatorMathBatch\n",
    "from dart_math.data import RespSampleVLLM, load_query_dps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [2024-06-01 17:10:13.990] [pid 409209] [/ssddata/tongyx/projects/dart-math/dart/utils.py:219:init_logging]\n",
      "log_path = None\n"
     ]
    }
   ],
   "source": [
    "init_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--swap_space'], dest='swap_space', nargs=None, const=None, default=60, type=<class 'float'>, choices=None, required=False, help='CPU swap space in GB for vLLM.', metavar=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"vLLM generation\", allow_abbrev=False)\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--gen_save_path\",\n",
    "    type=str,\n",
    "    required=True,\n",
    "    help=\"Path save results of generation (and evaluation).\",\n",
    ")\n",
    "\n",
    "# Device\n",
    "parser.add_argument(\n",
    "    \"--gpu_mem_util\",\n",
    "    type=float,\n",
    "    default=0.85,\n",
    "    help=\"GPU memory utilization for vLLM. Default: 0.85 in case of OOM.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--swap_space\", type=float, default=60, help=\"CPU swap space in GB for vLLM.\"\n",
    ")\n",
    "\n",
    "# Model\n",
    "parser.add_argument(\n",
    "    \"--model_name_or_path\",\n",
    "    type=str,\n",
    "    default=\"mistralai/Mistral-7B-v0.1\",\n",
    "    help=\"HF-style model name or path.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--dtype\",\n",
    "    type=str,\n",
    "    default=\"bfloat16\",\n",
    "    help=\"Data type for the model.\",\n",
    ")\n",
    "\n",
    "# Data\n",
    "parser.add_argument(\n",
    "    \"--datasets\",\n",
    "    type=str,\n",
    "    nargs=\"+\",\n",
    "    default=[\"math\"],\n",
    "    help=\"Dataset(s) for evaluation.\",\n",
    ")\n",
    "\n",
    "# Generation configurations\n",
    "parser.add_argument(\n",
    "    \"--temperature\",\n",
    "    type=float,\n",
    "    default=0,\n",
    "    help=\"Temperature for sampling.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--top_p\",\n",
    "    type=float,\n",
    "    default=0.95,\n",
    "    help=\"Top-p for sampling.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--max_new_toks\",\n",
    "    type=int,\n",
    "    default=2048,\n",
    "    help=\"Maximum number of new tokens.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--n_shots\",\n",
    "    type=int,\n",
    "    default=-1,\n",
    "    help=\"Number of shots for prompting. -1 means adaptive to datasets.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--prompt_template\",\n",
    "    type=str,\n",
    "    default=\"auto\",\n",
    "    help=\"ID / Path to the file of prompt template.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--n_paths\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"Number of generated completions per request. NOTE: might cause bug in vLLM (0.4.2).\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--save_gen_path_bs\",\n",
    "    type=int,\n",
    "    default=2**14,\n",
    "    help=\"# Completions = # Paths per request * # Requests. Values <= 0 mean adaptive.\",\n",
    ")\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--inf_seed\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"Random seed for inference. -1 means using us timestamp mod 2^32.\",\n",
    ")\n",
    "\n",
    "# Stopping criteria\n",
    "parser.add_argument(\n",
    "    \"--max_n_trials\",\n",
    "    nargs=\"+\",\n",
    "    type=int,\n",
    "    default=1,\n",
    "    help=\"(List of) maximum number of trials for each query. Non-positive means no limit.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--gen_only\",\n",
    "    action=\"store_true\",\n",
    "    help=\"Whether to only generate reponses and not evaluate the generated completions.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--min_n_corrects\",\n",
    "    nargs=\"+\",\n",
    "    type=int,\n",
    "    default=0,\n",
    "    help=\"(List of) minimum number of correct completions per query needed to stop generation. Non-positive means no goal.\",\n",
    ")\n",
    "\n",
    "args, unk_args = parser.parse_known_args(sys.argv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.inf_seed == -1:\n",
    "    args.inf_seed = int(time.time() * 10**6) % 2**32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirname = get_pathname_from_name_or_path(args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = (\n",
    "    get_prompt_template4model(args.model_name_or_path)\n",
    "    if args.prompt_template == \"auto\"\n",
    "    else PromptTemplate.load_from_id_or_path(args.prompt_template)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] [2024-06-01 17:10:14.87] [pid 409209] [/tmp/ipykernel_409209/488423047.py:5:<module>]\n",
      "Temperature is too small. Setting temperautre = 0, n_paths = 1, top_p = 1 for vLLM.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling_params = SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0, top_p=1.0, top_k=-1, min_p=0.0, seed=0, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=['### Instruction:', '### Response:'], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=2048, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None)\n"
     ]
    }
   ],
   "source": [
    "if args.temperature <= 1e-5:\n",
    "    args.temperature = 0\n",
    "    args.n_paths = 1\n",
    "    args.top_p = 1\n",
    "    logging.warning(\n",
    "        \"Temperature is too small. Setting temperautre = 0, n_paths = 1, top_p = 1 for vLLM.\"\n",
    "    )\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    n=args.n_paths,\n",
    "    temperature=args.temperature,\n",
    "    top_p=args.top_p,\n",
    "    max_tokens=args.max_new_toks,\n",
    "    stop=[prompt_template.query_prompt.strip(), prompt_template.resp_prompt.strip()],\n",
    "    skip_special_tokens=True,\n",
    "    seed=args.inf_seed,\n",
    ")\n",
    "\n",
    "print(f\"sampling_params = {sampling_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dps = load_query_dps(args.datasets, args.max_n_trials, args.min_n_corrects)\n",
    "for query_dp in query_dps:\n",
    "    query_dp.prompt_template = prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ssddata/tongyx/miniconda3/envs/dart-math/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-01 17:10:17 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='mistralai/Mistral-7B-v0.1', speculative_config=None, tokenizer='mistralai/Mistral-7B-v0.1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=mistralai/Mistral-7B-v0.1)\n",
      "INFO 06-01 17:10:18 utils.py:660] Found nccl from library /homes/tongyx/.config/vllm/nccl/cu12/libnccl.so.2.18.1\n",
      "INFO 06-01 17:10:19 selector.py:27] Using FlashAttention-2 backend.\n",
      "INFO 06-01 17:10:20 weight_utils.py:199] Using model weights format ['*.safetensors']\n",
      "INFO 06-01 17:10:23 model_runner.py:175] Loading model weights took 13.4966 GB\n",
      "INFO 06-01 17:10:25 gpu_executor.py:114] # GPU blocks: 25261, # CPU blocks: 30720\n",
      "INFO 06-01 17:11:12 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-01 17:11:12 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-01 17:11:17 model_runner.py:1017] Graph capturing finished in 5 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [2024-06-01 17:11:17.550] [pid 409209] [/tmp/ipykernel_409209/4134619846.py:11:<module>]\n",
      "LLM loaded!\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=args.model_name_or_path,\n",
    "    tokenizer=args.model_name_or_path,\n",
    "    tensor_parallel_size=len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(\",\")),\n",
    "    dtype=args.dtype,\n",
    "    seed=args.inf_seed,\n",
    "    gpu_memory_utilization=args.gpu_mem_util,\n",
    "    swap_space=args.swap_space,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "logging.info(\"LLM loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen(\n",
    "    llm,\n",
    "    sampling_params,\n",
    "    query_dps=query_dps,\n",
    "    dp_stop_criteria=is_dp_dars_finished,\n",
    "    resp_sample_cls=RespSampleVLLM,\n",
    "    batch_evaluator=(EvaluatorMathBatch() if not args.gen_only else None),\n",
    "    save_path=args.gen_save_path,\n",
    "    n_paths_per_save=args.save_gen_path_bs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"Generation done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
