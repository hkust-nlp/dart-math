{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "dev"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare batch file for OpenAI Batch API\n",
    "\n",
    "c.f. https://platform.openai.com/docs/guides/batch/1-preparing-your-batch-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import orjson\n",
    "import os\n",
    "from datasets import load_dataset, Dataset\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_info_list: list[tuple[str, str]] = [\n",
    "    {\n",
    "        \"id\": \"hendrycks/competition_math\",\n",
    "        \"split\": \"train\",\n",
    "        \"query_field\": \"problem\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"hkust-nlp/gsm8k-fix\",\n",
    "        \"split\": \"train\",\n",
    "        \"query_field\": \"query\",\n",
    "    },\n",
    "]\n",
    "temperature: float = 0.6\n",
    "n_sample_per_req: int = 2\n",
    "output_bs: int = 1000\n",
    "batch_home: str = \"../data/oai-batch-reqs\"\n",
    "task_tag = \"math-gsm8k-sys-patch\"\n",
    "os.makedirs(batch_home, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(covered_prompt_set)=12782\n"
     ]
    }
   ],
   "source": [
    "cover_fpath_list: str = [\n",
    "    \"../data/oai-outputs/output_t0.0_n1.jsonl\",\n",
    "]\n",
    "\n",
    "covered_prompt_list: list[str] = []\n",
    "for output_fpath in cover_fpath_list:\n",
    "    with open(output_fpath, \"r\") as f:\n",
    "        for line in f:\n",
    "            resp_sample: dict = orjson.loads(line)\n",
    "            if resp_sample[\"correct\"]:\n",
    "                covered_prompt_list.append(resp_sample[\"query\"].strip())\n",
    "covered_prompt_set: set[str] = set(covered_prompt_list)\n",
    "print(f\"{len(covered_prompt_set)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(todo_prompt_set)=1962\n"
     ]
    }
   ],
   "source": [
    "todo_fpath_list: list[str] = [\n",
    "    \"../data/oai-outputs/output_t0.3_n2.jsonl\",\n",
    "]\n",
    "\n",
    "todo_prompt_list: list[str] = []\n",
    "for output_fpath in todo_fpath_list:\n",
    "    with open(output_fpath, \"r\") as f:\n",
    "        for line in f:\n",
    "            resp_sample: dict = orjson.loads(line)\n",
    "            query: str = resp_sample[\"query\"].strip()\n",
    "            if not resp_sample[\"correct\"] and query not in covered_prompt_set:\n",
    "                todo_prompt_list.append(query)\n",
    "\n",
    "todo_prompt_set: set[str] = set(todo_prompt_list)\n",
    "print(f\"{len(todo_prompt_set)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(req_list)=1962\n",
      "req_list[0]={'custom_id': 'hendrycks/competition_math/train:6', 'method': 'POST', 'url': '/v1/chat/completions', 'body': {'model': 'gpt-4o-mini-2024-07-18', 'messages': [{'role': 'system', 'content': 'You are a helpful assistant. Solve the problem and provide the final answer in the format of \"The answer is $\\\\boxed{...}$\".'}, {'role': 'user', 'content': 'What are all values of $p$ such that for every $q>0$, we have   $$\\\\frac{3(pq^2+p^2q+3q^2+3pq)}{p+q}>2p^2q?$$ Express your answer in interval notation in decimal form.'}], 'max_tokens': 2048, 'n': 2, 'temperature': 0.6, 'top_p': 1.0, 'seed': 42, 'logprobs': True, 'top_logprobs': 20}}\n"
     ]
    }
   ],
   "source": [
    "req_list: list[dict[str, Any]] = []\n",
    "for dset_info in dset_info_list:\n",
    "    dset_id: str = dset_info[\"id\"]\n",
    "    dset_split: str = dset_info[\"split\"]\n",
    "    dset: Dataset = load_dataset(dset_id, split=dset_split)\n",
    "    prompt_list = dset[dset_info[\"query_field\"]]\n",
    "    for prompt_idx, prompt in enumerate(prompt_list):\n",
    "        if prompt.strip() not in todo_prompt_set:\n",
    "            continue\n",
    "        req_list.append(\n",
    "            {\n",
    "                \"custom_id\": f\"{dset_info['id']}/{dset_info['split']}:{prompt_idx}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": \"gpt-4o-mini-2024-07-18\",\n",
    "                    \"messages\": [\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": 'You are a helpful assistant. Solve the problem and provide the final answer in the format of \"The answer is $\\\\boxed{...}$\".',\n",
    "                        },\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": prompt,\n",
    "                        },\n",
    "                    ],\n",
    "                    \"max_tokens\": 2048,\n",
    "                    \"n\": n_sample_per_req,\n",
    "                    \"temperature\": temperature,\n",
    "                    \"top_p\": 1.0,\n",
    "                    \"seed\": 42,\n",
    "                    # Maximize information gains\n",
    "                    \"logprobs\": True,\n",
    "                    \"top_logprobs\": 20,\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(f\"{len(req_list)=}\")\n",
    "print(f\"{req_list[0]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req_bs: int = output_bs // n_sample_per_req\n",
    "for split_start_idx in range(0, len(req_list), req_bs):\n",
    "    split_end_idx = min(split_start_idx + req_bs, len(req_list))\n",
    "    with open(\n",
    "        f\"{batch_home}/req_{task_tag}_t{temperature}_n{n_sample_per_req}_{split_start_idx}-{split_end_idx}.jsonl\",\n",
    "        \"w\",\n",
    "    ) as f:\n",
    "        for req in req_list[split_start_idx:split_end_idx]:\n",
    "            f.write(orjson.dumps(req).decode(\"utf-8\") + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
